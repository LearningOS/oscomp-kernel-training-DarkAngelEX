# 决赛总结

此文档写于比赛结束之后，意在全局性地回顾并反思FTL OS的设计，并希望后来的队伍少走弯路。这并不是主要原因，最主要的原因是追求性能的项目组怎么比我们预想的要少这么多？我们卷到极致的开发方式就像刀刃打在棉花上，完全凸显不出FTL OS有多么地厉害。为了提高接下来队伍的综合实力，让各个队伍再卷一些，增加对抗性，献上此文。

接下来的性能开销都为U740平台数据，它的性能约为K210的4-8倍。

## FTL OS的弯路

我们在设计之初就确认了FTL OS以性能为第一目标，也因此完全从零开始设计了操作系统。许多队伍通过各种方式来寻找性能瓶颈，并针对性地提高操作系统性能。我们是这么思考的：**只要我们用了一切可能的优化，就不存在能够超越我们的队伍。**基于这个思想，我们从初赛时就开始进行细节性的架构设计，从Linux等渠道积累优化方式并尽早应用，编写文档，调试BUG。这个开发方式从总体来看是成功的，它让FTL OS在决赛第二阶段的大部分时间里以超过3分的巨大优势领先第二名，如同大魔王蔑视下方的勇者，并在最后依然拥有0.8分的性能优势。

只看结果，0.8听起来不是很多？FTL OS也这么认为，因为按照计划领先优势应该在2分以上，这源于过于超前的开发导致的设计失误。FTL OS的代码量达到了其他任意一个项目组的3倍以上，这3倍的代码量只有很少一部分会产生实际的性能效益，大部分属于过度设计。过度设计的一部分原因是我们没有详细地调研现代操作系统的实现，更深层的原因是执行力过于强导致文档和代码编写完成得太早，然后就将精力投入其他领域，不再关心已完成的模块。

许多过度设计不会对操作系统产生影响，因为大部分是针对“容器”的优化，例如我们编写了一大堆不同种类的锁，一大堆看起来很厉害的容器。尽管它们的实现各不相同，但由于接口相同，我们想将新优化上线时只需要改变一个类型名，如果是负优化也可以方便地撤销，冗余代码放在那就好。但一旦过度设计涉及架构部分，情况就变得非常地棘手，FTL OS就在文件系统的设计上多次遇到此问题。

## 文件系统

文件系统听起来是操作系统的细枝末节，它甚至可以被完成抽象出来并独立于操作系统，这么简单的东西怎么会发生过度设计呢？尽管不想推卸责任，但它确实主要是rCore-tutorial导致的。rCore-tutorial的文件系统非常简化，它的缓存块数量近似于0；每次写入数据都会同步到磁盘，导致缓存块和没有差不多；它不存在虚拟文件系统这一抽象，没有inode层面的缓存。这些导致了凡是使用rCore-tutorial的文件系统架构的队伍都或多或少遇到性能问题或设计问题，最好的解决方案就是抛弃它重新写一个更好的。

FTL OS自然从一开始就不会沿用rCore-tutorial的文件系统，毕竟已经明确了要使用异步驱动，文件系统本来就是要重写的。不过我们的文件系统依然重构了好几次，这简直是奇耻大辱。我们的重构全部是由于调研不完善导致的，它经过了如下几个过程：

* 第一版。只缓存块缓存，完全没想到可以在inode层面缓存，所有操作全部在块缓存层面进行，导致所有权以及锁顺序等陷入混乱。
* 第二版。第一版完成的一星期后发现居然可以在inode层面缓存！于是文件系统基本推倒重来。同时使用了基于强弱指针的缓存方式，细节见文档。
* 第三版。什么，世界上居然有个叫虚拟文件系统的东西？？？好吧，操作系统课上学过。但是回顾前面的代码，我们发现我们居然在FAT32文件系统这一磁盘文件系统上干了虚拟文件系统的活！重构太累了，我们放弃重构，直接编写了虚拟文件系统，并将之前写的FAT32文件系统直接挂载到虚拟文件系统上。幸好虚拟文件系统写得晚，在RCU系统完成之后，因此可以深度融合RCU设计，我们的虚拟文件系统的性能达到了全部队伍的第一。
* 第四版。什么？只要是空目录就可以删除，就算是目录文件没有关闭也行？什么？文件可以提前被删除，删除后可以接着用？
* 第五版。什么？Linux居然有个叫页缓存的东西，共享映射直接指向它？那对齐怎么办？什么？文件映射的偏移必须是对齐到页的？。。。之前文档看错了，还以为文件映射的偏移是允许不对齐的，结果只是映射的长度可以不对齐啊！页缓存才是实现文件共享映射的正确方法，这个方法让一个文件不管被多少个进程映射都只存在一个副本，并且能非常方便地使用同步机制。
* 第六版（虚构）。有没有发现，页缓存这东西...它就是一堆缓存块。既然如此，我们FAT32文件系统里的缓存是做什么用的？为什么要给已经存在的缓存块再增加一层？（参考Linux页缓存与块缓存）

好多迭代啊，FTL OS真是太努力了...这种努力除了感动自己以外没有任何意义。如果按照FTL OS的设计思想，我们应该在初赛阶段就直接完成第五版设计，然后一步到位完成最终版文件系统。但事实就是FTL OS走了一大堆弯路，进行了一大堆过度设计，产生了一大堆费码，最终execve的性能居然还打不过西北工业大学（因为FTL OS仍处于第一，第五版设计只在direct_map分支完成了代码编写，没有提交到评测机）。

## 文件系统：如果再来一次

如果FTL OS本着完成梦想的心态再参加一次比赛，那么它的文件系统应该是这样的：

### FAT32文件系统设计

* **不要让磁盘文件系统干虚拟文件系统的活！**
* FAT32文件系统不需要进行太多优化，它唯一的作用就是从磁盘读取数据，或将数据写回磁盘。
* 簇链缓存是必须的，否则FAT32文件系统的随机访问时间复杂度会达到O(N)。可以使用懒加载的方式，读到哪就缓存到哪即可。
* 缓存小细节，比如目录项的位置、长度等。
* 由于块缓存在虚拟文件系统层实现，因此需要通过某种方式访问到虚拟文件系统的块缓存，例如函数参数。
* 文件系统可以假设任何文件只会被外部持有一个引用，它由虚拟文件系统保证，这样就不需要思考麻烦的数据竞争了。
* 分离文件的**目录树删除**与**数据删除**两个操作，因为Linux允许文件从目录树中删除后继续被使用。

### 虚拟文件系统设计

* **虚拟文件系统负责从目录树到数据缓存的一条龙服务。**
* 虚拟文件系统会挂载具体的文件系统，同时维护一棵不完整的目录树，里面只存在被缓存的目录或文件，统一称为缓存的inode节点。
* 设计阶段融合RCU设计，所有缓存节点都使用RCU释放，使用额外的标志位来标识状态。
* 使用LRU算法淘汰缓存的inode节点。虚拟文件系统包含一个未使用链表，链表上的节点不存在外部引用。如果它被外界重新获取就会移除链表。
* 每个缓存inode节点具备虚拟文件系统层面的**页缓存**。页缓存的大小通常比文件系统的块大小要大，因此一个页缓存可以被划分为多个**块缓存**，所有对文件的直接操作都在这一层面上进行。例如read、write等操作，它们很可能不必操作整个页，可以按块或页加载或写回来减少IO次数。如果要进行文件映射，就把这个页中其他未加载的块给加载了。写回非常重要，仿造Linux使用一个定时的内核进程来异步写回脏块。只有不存在可写权限内存映射的页面才需要写回。
* 将缓存操作传给具体文件系统。

### 内存文件系统

* 内存文件系统用来加速文件的创建和删除，针对10个文件系统测试。
* 目录要使用哈希表，不要用BTreeMap，因为测试中单个目录上会被创建超过100个文件并写入或删除数据，哈希表的O(1)优势非常明显。

这个设计并不复杂，且一步到位地实现了页缓存并保证了内存映射与文件数据操作的同步。它根本不存在迭代开发的需要，按FTL OS的开发实力，我们可以在5天内完成上述设计的编写、调试、上线，在初赛阶段就让其他队伍陷入绝望。

## 如何将每项评测做到第一

决赛第二阶段通过基准测试测试操作系统性能。它具有一系列测试点，每个测试点具有一个基准值。第二届比赛中，每个测试点的得分如此计算：
$$
Score = 2-\frac{1}{max(1,K_{speedup})}
$$
加速比K是操作系统性能和基准值的比值，通过延迟或吞吐量计算。只要通过了测试点就能获得1分的保底分，如果操作系统性能远超基准值也最多得到2分。当然基准值的选取不太科学，个人认为基准值是拿4GHz的intel处理器跑出来的，很多测试点的基准值根本没有到达的可能。此制度下，2分已经是非常大的差距了。个人认为这个评分方式本来就不太科学，尽管相比第一届的K值就是分数要合理不少，但这个保底也太暴力了，低于保底性能没有任何区分度。个人认为这个就不错：
$$
Score = 2-\frac{1}{1+K_{speedup}}
$$
此方法具备保底的1分，基准性能时斜率为0.25，极限得分为2，完成测试是性能比拼的前提，性能低于基准值也有区分度。

最重要的还是基准性能的合理性，基准性能不改天理不容，怎么可以用4GHz的x86-64架构处理器性能来衡量400MHz的RISC-V架构的k210呢？应该在实际开发板上测试性能，或者直接设置为历史比赛中此此项目第一名的值。不过某些项目可以通过舍弃其他项目的方式来提高性能，例如放弃mmap的懒执行就可以大幅提高pagefaults的性能，需要补偿方案，例如取第一名和Linux标准基准值的平均数。

接下来以FTL OS自身为例，手把手教你如何将每个评测点的性能做到极致。

### 全面COW

任何操作系统课都会讲解COW，因为COW会全方面带来恐怖的性能提升。但是如果要将性能分拿满，全方面地实现COW是必不可少的，你需要把.data段、匿名文件映射、栈全部都在fork时COW。

实现COW很简单，但是COW是有局限的，它的引用计数的原子开销非常巨大。如果要进一步提升性能或在K210平台绕过页面交换，你需要实现基于页缓存的文件映射，并应用于lazy-execve。

### ELF解析

FTL OS采用了rCore-tutorial中使用的xmas-elf库来解析elf文件。这个库功能强大，但有个致命缺点，你必须把完整的文件给放入一块连续内存。情况不妙，有一个测试会计算execve的调用延迟，降低延迟必须实现懒执行的execve。但这么做就不能加载整个代码文件，因此必须要替换或重写xmas-elf库。objects库是个不错的选择（FTL OS的异步文件系统用不了...）。

替换或重写xmas-elf时不必太过在意性能，懒执行的execve占用的时间不会超过5%，绝大多数时间花费在缺页处理中，execve自身不会成为性能瓶颈。

FTL OS的文件映射接口设计得比较合理，我们直接使用了文件映射接口来映射代码文件。

#### execve小细节

大多数人都写过shell脚本，它通常以.sh结尾。实际上它是可以被execve给执行的，execve会检测文件的开头，如果它包含`#!aa/bb`，execve就会调用`aa/bb`，并将原文件作为第二个参数。详情可见execve的文档。

FTL OS参考健康向上好青年队用了一种更简单暴力的方案，就是判断文件名是不是.sh结尾，如果是就直接转发给busybox。

### 文件页缓存

文件页缓存已经被Linux广泛使用，在本次比赛的U740赛道，西北工业大学的NPUcore队首先实现了文件页缓存，并称其为零拷贝技术。称其为零拷贝技术也是合理的，因为虚拟内存直接映射到了页缓存中，没有任何的复制过程。但NPUcore似乎只对代码文件进行了文件页缓存，没有更进一步，将它扩展到全部文件映射上。健康向上好青年队随后实现了文件页缓存并同时应用于文件映射，达到了execve和文件映射相关的测试程序的最高性能，性能比FTL OS使用的多核预读技术还要高。

共享文件映射应该使用页缓存，这不是优化，这是唯一正确高效的方法，我知道你们都不想做文件映射页面写回（建议增加检测文件映射数据同步的测试）。个人认为如果不使用页缓存技术，根本不可能同时实现高性能、多进程数据同步、写回。

页缓存技术不需要使用大量的引用计数来管理每一个页。因为只要文件存在缓存就存在，我们只需要递增“文件映射的引用计数”，当文件映射引用计数存在时禁止释放页缓存，每个段只需要一个引用计数。这种方式下页缓存完全不存在引用计数开销，因此它比COW还要快，同时因为不需要所有权页面，它不需要复制任何页面，不会占用太多的L1 cache，因此不要对页缓存页面使用COW的负优化。从比赛结果来看，使用了页缓存技术可以把execve延迟降低到500us，不使用页缓存的极限延迟是1000us。它还可以显著提高pagefaults、lat_mmap的性能（3倍以上）。

设计时一定要为页缓存留出接口。页缓存的速度将超过常规方式的10倍，涉及测试极广，不要妄想在其他方面把性能追回来。

页缓存技术深度绑定文件系统，不要在决赛后期才考虑实现页缓存。

#### 代码文件映射的小细节

* 不是整个代码文件都可以进行页缓存映射的！只有无写权限的代码段才能页缓存映射。
* .rodata在加载时具有写权限，但我们依然可以映射到页缓存，libc会在初始化完成后去除它的写权限。
* 即使是拥有写权限的页面依然可以映射到页缓存。听起来是不是不太安全？我们可以这样处理：未映射的页面如果被读取，那么进行页缓存映射，如果被写入，那么复制到新的页面。
* 文件映射的起始偏移量一定对齐到页，如果不对齐可以抛出EINVAL错误。但映射的长度可以不对齐，我们需要将超出的部分填充0。可能存在一种页面，它的一部分需要从文件加载，另一部分填充0，如果它不是共享映射，我们必须给它分配一个独立的内存，别把共享缓存给填充0了！

由于文件系统的设计失误，比赛中FTL OS的页缓存系统没有上线，execve的性能屈居第三，累加损失约1性能分。

### 统一的线程睡眠唤醒机制

FTL OS采用了无栈协程架构，rust采用了reactor异步模型来实现它，用`Future`+`Waker`的方式来统一协程的睡眠与唤醒。

下一届比赛有栈架构必定继续占据主流位置，但有栈协程无法使用rust提供的异步模型。如果缺少了统一的睡眠唤醒机制，waitpid、read、write等系统调用就会让人头皮发麻，除非自暴自弃地用`yield`（rCore-tutorial看看你带的头）。睡眠唤醒机制将深入操作系统的各个部分，后期重构的工作量非常大，因此应该尽早明确它的设计。

如果决定使用多核，请确保你的调度器能够在多核环境下正常运行！请参考[进程调度-任务调度器](./进程调度-任务调度器.md)的第一部分。

xv6实现了简单有效的睡眠唤醒机制，但调度器的唤醒复杂度是O(N)（N是进程数量，包括睡眠进程）。FTL OS使用了O(1)的FIFO调度器，无论睡眠或唤醒。侵入式链表是降低调度器时间复杂度最简单的方式，一个进程控制块可以还可以挂上一大堆不同的节点，同时应用不同的调度方法。

### 多核

多核可以显著提高性能，相比单核简直是降维打击。即使测试程序看起来是单线程的，多核依然可以隐藏许多延迟，提高速度。

最简单的例子，一个进程退出时需要释放自身的页表，还需要往父进程发送SIGCHLD信号。普通做法是先释放自身页表，再发送信号。但是如果先发送信号再释放页表，父进程就可以在释放页表之前启动，释放页表的延迟就被隐藏了。释放页表的操作应该在退出时主动进行，如果放在析构函数中执行，那么释放页表的开销就被转移到了父进程的waitpid，延迟就无法隐藏了。此优化在U740可以降低约40us的开销。

如果你下定决心不使用多核，那么就不要使用`Arc`智能指针了，你不需要它的原子指令。可以考虑封装`Rc`并添加`Send`和`Sync`的trait，它的复制与析构速度是`Arc`的10倍以上。

如果你要实现多核运行：

* 原版rCore-tutorial不可能多核运行，它在设计上就从来没有考虑过多核运行的可能。
* 单纯地把全部`UnsafeCell`改成`Mutex`也无法跑起来，它的调度逻辑根本无法在多核下正常运行。
* 你需要实现一套多核环境下能够正常运行的调度机制，可以参考[进程调度-任务调度器](./进程调度-任务调度器.md)的第一部分。
* 越早越好，如果拖到决赛再实现多核，你会发现各种需要睡眠的系统调用都得重写，享受到我们重构文件系统时的痛苦。

### 缺页预测

缺页预测是FTL OS针对一些分数接近的队伍的反制手段。如果他们在最后阶段不讲无德地偷袭了FTL OS，FTL OS将使用这非常靠谱的缺页预测地给予他们绝望。但最终缺页预测依然躺在发射架里没有应用。

FTL OS设计了两种缺页预测。第一种是针对fork的缺页预测，它通过历史预测fork发生后最近发生的10次COW缺页，提前复制页面，让每次fork约减少15us的开销。

第二种才是真正的核武器，它可以把某些系统调用测试中把文件映射延迟降低到近乎作弊的耗时（小于进入内核态的耗时），同时不会影响操作系统的其他部分，这就是连续页面缺页预测。我们可以通过预加载的方式提前加载多个连续页面，将进入内核态的耗时给均摊到每个页面上。它基于这样的猜测：

> 如果历史缺页是连续的，那么未来的缺页很可能依然是连续的。
>
> 未来的缺页应该和发生的连续缺页处于同一个数量级。

实现也非常简单：

* 每个进程持有一个缺页预测器，它保存了下一次预测的缺页和连续缺页的次数*N*。
* 当发生缺页时，如果和预测的缺页位置不同，则连续缺页次数归零。
* 当发生缺页时，如果和缺页的位置相同，则映射从缺页位置开始的*f*(*N*)个页面，并将连续缺页次数更新为*N*+*f*(*N*)。如果区间中出现了很多已经映射的页面或者存在无法映射的页面，连续缺页次数将归0。

*f*(*N*)可以通过实验来获取，或许*f*(*N*)=min(0.5*N*, 20)是个不错的函数。lat-mmap和pagefaults测试会连续地映射128个页面，上述方法只需要进入内核态15次就可以完成全部映射。对于随机情况，此函数从第四次开始才会映射超过1个页面，不会发生过多误预测的情况。

本次比赛中北航的图漏图森破队的pagefault测试延迟为0.0508us，而它们simple syscall的开销为0.7509us。它们的开销这么低是因为没有使用lazy mmap，pagefault测试时根本没有发生缺页，从lat_mmap的开销是3034us可以看出来。缺页预测技术可以在保有lazy mmap的低延迟的同时将pagefault开销降低到北航的水平。

### 管道

在比赛中管道的性能有几个梯队：800MB/s，400MB/s，40MB/s。FTL OS是唯一达到第一梯队的操作系统，速度为865MB/s。虽然增加的分数不多，但多一点总是好的。

达到第二梯队非常简单，简单的扩大缓存即可。但你最终会发现，即使缓存设置得再大，依然无法突破500MB/s的天花板。

FTL OS为什么这么快？因为我们的管道允许读写并行，没有暴力地用互斥锁把缓存锁起来，原理见[同步系统-进程间通信](./同步系统-进程间通信.md)的最后部分。这其实是wait-free技术的应用。

### 上下文切换测试

上下文切换速度分值不少，如何提高性能？

* 让拿不到数据的管道睡眠！不要再用愚蠢的yield了！
* U740单核运行测试的极限性能约为8us，如果要进一步提高速度，必须使用多核运行。
* 如果上下文切换速度足够快，双进程切换速度可以达到2us的速度，此时两个进程分属两个核执行，根本不发生上下文切换。
* 多核运行的切换速度可以达到6us，因为切换点是“唤醒”而不是本进程睡眠，唤醒之后的延迟被隐藏了。
* 多核提高性能的原因不止是切换，另一个原因是多核拥有更多的L1-cache。这些进程运行的函数是相同的，访问的虚拟地址也大致相同，全部命中于同一个组相联，导致了cache竞争，多核可以缓解这种情况。
* U740平台上64进程和96进程测试中慢得众生平等（30us与60us），这可能是L2 cache的16路组相联失效导致的。不过为什么32进程测试只用了6us？

### 关于原子指令

`Arc`智能指针通过原子计数实现，原子操作听起来就很慢，但真的如此吗？

不能通过通常的性能分析方法来看待原子指令，因为它的开销涉及MESI缓存一致性协议。每个页面都具有唯一、共享的状态，如果原子指令发生在唯一状态的页，它的开销很小，约为10-40个时钟；但如果页面处于共享状态，那开销就上天了，可能要花费上百个时钟。与此同时，原子指令通常和内存屏障一同使用，因此将伴随着大量的脏页面写回，这部分开销也很大。

当然原子操作还是越少越好，在大多数情况下，把函数参数里的`Arc<T>`替换为`&T`或`&Arc<T>`就可以提高性能，因为减少了一次原子开销。

### 内存相关

很多内存并不需要清零，例如内核栈。它刚被分配时就是空的，为什么要清零呢？

有些内存是要清零的，例如页目录项。尽管测试程序不要求，但未清理的页目录项是非常危险的，用户可以访问到错误的物理地址。但如果能保证释放页表时页目录项不存在有效页表项，我们可以把它们缓存起来，这样分配时就可以直接获取清零的页表了。

用户程序会需要全零的页面，如果你使用了多核，可以尝试全零页面预写，让一些空闲的核心提前往一些页面写入0，分配时就不需要清零了。

如果要用智能指针管理4KB的页面，不要用`Arc<[u8;4096]>`，请用`Box<[u8;4096]>`或`Arc<Box<[u8;4096]>>`。`Arc`需要在内存的最前面放置两个强弱引用计数，总共需要4096+8+8=4112字节的空间。听起来问题不大？大多数队伍都使用伙伴分配器，它只能分配2的幂次的空间，因此实际上会分配8KB的空间。这还毁掉了伙伴分配器的对齐性质，由于前面多了两个引用计数，这一块内存的起始地址只对齐到了16字节，不能用于页表映射。

### 内存交换（仅K210）

K210平台可以控制至多8MB内存，如果你没有实现文件映射的页缓存或内存交换，100%内存不足。但即使实现了页缓存依然有内存不足的可能，但已经可以通过大多数测试。懒执行和页缓存是节约内存最有效的方案。

上下文交换测试中每个进程会使用最多16KB的栈空间，这么大的内存是被vfprintf系列函数使用的。vfprintf函数为了不使用动态内存分配，在栈上使用了巨大的缓冲区（其实也不算太大）。测试进程运行时不会输入任何信息，自然用不到vfprintf。因此如果全方面应用了COW，每个进程事实上只会使用4KB的活跃栈，96个进程需要约400KB的空间，其他空间全处于共享状态。如果没有实现COW，它就变成1.6MB了，一下子就占用了五分之一的内存。这只是栈占用的内存，再加上动态内存分配的mmap，data段的数据...这几分不用要了。

由于U740平台内存太多了，FTL OS根本没有思考过去实现内存交换。

## 高效开发

如果你花费了大量的时间来重复地干初中生都会的事情，干完后还沾沾自喜，那么你很可能陷入了感动自己的循环，除了浪费时间外毫无意义，因为有更高效的方法来解决它，让我们把时间真正地投入有意义的事情上。

### 快速弄清楚每个宏的值

用户程序通过系统调用来和操作系统交互。很多系统调用具有很多选项，用户可以用各种标志位来告诉操作系统要干什么，例如我们在实现openat系统调用时需要检测`O_CREAT`。现在问题来了，这个宏`O_CREAT`到底是多少？它是平台相关的，在不同平台上取不同的值。Linux里这些奇奇怪怪的宏定义如天女散花般混乱，如何高效地得知它是多少？

推荐一个网站：[Compiler Explorer (godbolt.org)](https://gcc.godbolt.org/)，选择C++语言，编译器选择`RISCV rv64gc clang (trunk)`，然后就可以直接写代码了。

首先在[Linux kernel system calls for all architectures](https://marcin.juszkiewicz.com.pl/download/tables/syscalls.html)里找到openat并点击，进入了`open(2) — Linux manual page`，这个网站也是我们最常用的查找文档的网站。arm64和riscv64的调用号是一样的，不需要每次都去拖动遥远的riscv64条目。

在`SYNOPSIS`这一栏里发现它有`#include <fcntl.h>`，我们把它粘贴到Compiler Explorer里面。

随便写一个返回size_t的函数，它返回`O_CREAT`，同时在编译选项中增加`-O3`。

```C
#include <fcntl.h>
size_t getnum() {
	return O_CREAT;
}
```

等待线上编译完成。结果如下：

```assembly
getnum():       # @getnum()
    li  a0, 64
    ret
```

`O_CREAT`的值就是64。这个方法方便又快捷，还能保证绝对的正确。

### 内存错误DEBUG

内核态异常由内存错误导致，一般只会有如下来源：没刷表，野指针，二次释放，多核不安全，中断嵌套。

#### 没刷表

只有使用了用户态和内核态共享地址空间且在trampline中不刷表的设计才会出现这个问题。鉴别它非常简单，把刷表指令加回去，错误消失了就说明错在刷表。

需要注意，刷表指令只对单核有效。在多核情况下，你需要设计一套安全合理的多核刷表方案，或者使用非常非常缓慢的sbi代理的`remote_sfence_vma`指令。

别忘了`fence.i`！不过没必要插得到处都是，它的开销还是比较大的。

#### 野指针/二次释放

鉴别方法很简单。当释放内存时，用如`0xfe`等数字把内存给覆盖了，尽早暴露错误。当分配内存时也可以检测内存，如果它某处值不等于覆盖的数字，说明它存在野指针。

也可以尝试关闭内存释放，即泄露所有内存。如果操作系统不再出错，这也说明存在问题。

使用rust提供的`Box`或`Arc`能杜绝发生这种错误的可能。如果要使用侵入式链表+RCU的结构，就需要好好考虑了。

#### 多核不安全

调试非常困难。但确定的方式很简单，即只用单核运行，如果BUG消失了就说明多核存在BUG。

多核安全中最常见的问题是死锁。我们应该设计一个尝试检测机制，如果获取锁的次数超过了一千万就说明存在死锁。即使不存在死锁，这么多的尝试次数也说明架构设计存在缺陷，应该用睡眠锁替换或重新设计。

#### 中断嵌套

每一个可能在中断处理程序中使用的部件都需要仔细考虑，最常见的例子就是各种per-CPU结构，例如per-CPU内存分配缓存。绝大多数容器发生中断嵌套都会出错，包括最常见的`Vec`。访问它们必须关中断，或者保证它不会在中断上下文中被使用。

当然也大可自暴自弃，在内核态全程关闭中断，这会影响一些内核态定时执行任务的设计，不过总体影响不大。