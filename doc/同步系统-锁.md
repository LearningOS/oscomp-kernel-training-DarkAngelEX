# 锁

FTL OS实现了自旋排他锁，自旋共享锁，睡眠排他锁，睡眠共享锁，顺序锁等同步方式，分别应用在不同的场景。

## 自旋排他锁

FTL OS的自旋排他锁实现非常简单，核心代码如下：

```rust
// ftl-uitl/src/sync/spin_mutex.rs
fn obtain_lock(&self) {
    while self.lock.compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed).is_err() {
        while self.lock.load(Ordering::Relaxed) {
            core::hint::spin_loop();
            ... // deadlock detect
        }
    }
}
fn drop(&mut self) {
    self.mutex.lock.store(false, Ordering::Release);
    S::after_unlock(&mut self.guard);
}
```

`obtain_lock`使用CAS操作更新原子变量`lock`，当值为false时原子地修改为true，这是自旋锁实现的通用方法。`drop`函数中将`lock`写入false，这两处代码是唯一会修改`lock`字段的代码，而当`lock`为true时obtain_lock无法修改`lock`，因此析构函数对`lock`的修改一定会被`obtain_lock`观测到。CAS操作失败时执行的等待操作为普通的访存，自旋等待时不会如CAS操作一样反复广播原子操作导致性能下降。FTL OS设计上不存在长时间占有的自旋锁，需要长时间占有对象应该由睡眠锁保护。因此FTL OS在自旋次数过多时将触发panic，这意味着内核发生了死锁。

## 自旋读写锁

FTL OS自旋读写锁是读者优先锁，状态定义如下：

|   lock的值    |                 定义                 |
| :-----------: | :----------------------------------: |
|     -2^31     |                非法值                |
|    -2^31+1    | 读写锁被写者占有，没有正在等待的读者 |
| [-2^31+2, -1] |  读写锁被写者占有，有正在等待的读者  |
|       0       |              锁未被占有              |
|  [1, 2^31-1]  |  读写锁被读者占有，数量等于lock的值  |

FTL OS的读者在获取读锁时会直接使用`fetch_add`操作将lock的值增加1，即使锁处于写锁状态也进行操作，然后才等待写者将锁释放。这样的实现相比在写者释放后进行原子操作将原子开销隐藏在了写者占有锁的时间段内。相比于排他锁，读写锁可以为读者提供更高的并行度，但释放锁时也需要一次原子操作，两者开销需要权衡。

## 睡眠锁

自旋锁可以在多核环境下有效保护数据的安全，但在竞争严重的环境下会导致CPU资源的极大浪费。而睡眠锁在获取锁失败时将出让CPU，可以有效地提高CPU资源的利用率。为了深度融合rust的async架构，FTL OS自行实现了异步睡眠锁，当睡眠锁结束占用自动唤醒下一个等待的任务，唤醒顺序严格等于提交顺序，且运行时不需要分配任何内存。

睡眠锁包含三个部分：睡眠锁本体，睡眠锁等待器，睡眠锁追踪器。等待器和追踪器对应了睡眠锁上锁的两个阶段：等待阶段和持有阶段。而等待阶段也被划分为了两个部分：初始化阶段和唤醒阶段。在初始化阶段睡眠锁将产生一个等待器，如果在初始化时睡眠锁处于上锁状态，等待器会将自身注册到睡眠锁的等待队列中并出让CPU。当持有睡眠锁的任务释放它的追踪器时会从睡眠锁等待队列中唤醒一个任务。被唤醒的任务将进入持有阶段，获取睡眠锁追踪器并释放等待器。相比于Linux的睡眠锁，FTL OS在唤醒阶段不需要获取睡眠锁的链表锁，被误唤醒时开销极小。

![image-20220525174301441](pic/锁1.png)

为了实现不需要额外分配内存的睡眠锁，睡眠锁将等待器放置在“逻辑栈”上。所谓“逻辑栈”是因为等待器跨越了`await`，空间被分配在了等待器所属`Future`的堆上，但我们依然可以将它看作在“逻辑栈”上，它所需的内存在所属`Future`分配内存时被一同分配了，没有额外内存分配。

等待器上包含一个侵入式链表节点，初始化时链表节点将会添加到睡眠锁等待队列的链表末尾。等待器上还携带了表示是否允许运行的标志位，这个标志位可能在初始化时被设置为true表示睡眠锁为空，不需要阻塞，但如果初始化时被设置为false，那么它只能被离开睡眠锁的任务设置为true。为了能够被释放锁的任务唤醒，等待器上还放置了自身的唤醒器，使用唤醒器可以将对应的任务加入调度队列。需要注意的是等待器获取锁后就析构了，其他任务再获取它的任何数据都是极为危险的，而等待器能够析构的标志就是运行标志位，因此所有涉及唤醒任务内存的操作都需要在标志位修改之前进行。因此唤醒器需要在标志位修改之前取出。但任务的唤醒必须在标志为true之后，否则由于确实内存序限制，被唤醒任务执行poll时可能依然观测到运行标志位为false导致唤醒失败，任务丢失，而这又导致此睡眠锁死锁，而睡眠锁是无法用尝试次数的方式来发现死锁的。正确的方式是在标志位修改之前取出唤醒器，之后修改标志位，最后再用唤醒器唤醒对应任务。FTL OS调度器保证了如果任务被唤醒时处于运行状态，任务结束运行状态时会被再次放入调度器，不会导致任务丢失。

## 内存序

存粹的原子指令只能保证自身的修改在多核上是原子的，但被两次原子指令包围的起来的访存操作是如何保证在多核可见的呢？这就涉及了内存序。C++20标准定义了如下内存序：

| 内存序  | RISC-V 实现  |                             描述                             |
| :-----: | :----------: | :----------------------------------------------------------: |
| relaxed |    (NULL)    |                      不对访存作任何约束                      |
| acquire | fence r, rw  |            禁止之后的所有读写操作被重排序在此之前            |
| release | fence rw, w  |            禁止之前的所有读写操作被重排序在此之后            |
| consume | fence r, rw  | 类似acquire，但只对release的数据依赖链生效，不作用于全部访存 |
| acq_rel |  fence.tso   |               同时具有acquire和release的内存序               |
| seq_cst | fence rw, rw |                    操作将在全局具有顺序性                    |

除了relaxed内存序外所有内存序都禁止了读操作重排序与CPU内部相关数据的乱序执行。需要注意的是单独使用acquire或release并不能保证数据的安全性，只有release-acquire结合使用才能保证数据操作是安全的。rust目前尚不支持consume内存序。

CPU是如何保证所有写操作在多核间可见的？多核CPU采用**MESI缓存一致性协议**，每个缓存块都有无效、独享、修改、共享四种状态。独享状态的数据和内存是一致的，当其他CPU读取时会变为共享状态，发生写入时变为修改状态。修改状态缓存的数据与内存是不一致的，当其他核进行读操作时会写回主存并转变为独享状态。共享状态的数据被多个缓存持有并和主存是一致的，只要任何CPU修改了缓存，其他CPU对应的缓存都变为无效状态。MESI保证了任意数据的写入都能够在未来被所有其他的核心观测到，而内存序对应的内存屏障为状态的更新提供了顺序保证。

MESI协议的操作单位是cache-line，绝大多数平台上这一大小为64字节，这导致了被称为**伪共享**(false share)的性能下降现象。处于同一个cache-line的对象会共享缓存状态，因此当共享对象被其他核修改时整个cache-line都会失效，例如一个数组中每个单元都属于各自的CPU，CPU在各自单元上的操作理论上的互不影响的，但由于共享了cache-line会导致大量的cache失效，在高频CPU上会导致5倍以上的性能损失。更常见的例子是引用计数，由于引用计数是于数据一起放置的，因此每次引用计数的更新都会导致对象cache失效。为了避免伪共享，线程局部对象需要对齐至cache-line。

Hifive Unmatched使用的U74核心只有一个访存端口的顺序处理器，但这不意味着内存序不会出错。包括U74在内的多核CPU采用了如下的两种方式来提高性能：

* 写缓冲区（Store Buffer）。MESI协议的写操作将等待其他CPU的确认信息，在这期间写操作将被阻塞。为了避免阻塞影响到CPU，所有store操作会先提交至写缓冲区，在之后的适当时机才会写入cache与内存，而只有写缓冲区满时才会阻塞CPU。写缓冲区避免了写操作导致的频繁CPU暂停，但也导致在其他核心看来这个写操作被延迟了，如果延迟至了某条load之后就产生了store-load乱序。release内存序生成的屏障指令可以阻止它的发生，`fence r, rw`执行后的任何访存操作会等待写缓冲区清空，保证其他核心的观测顺序。
* 失效队列（Invalid Queue）。MESI协议的读操作需要等待其他CPU导致的缓存行被清空后再执行，在这期间读操作将被阻塞。引入失效队列后CPU的读操作不再被阻塞，其他CPU发送的缓存行无效信息将立刻回复并加入失效队列，在未来的适当时机再刷入cache。失效队列能有效提高其他CPU写操作的速度与当前CPU读操作的速度，但也导致了load-load与load-store乱序。acquire内存序生成的屏障指令可以阻止它的发生，`fence r, r`之后的访存操作将阻塞等待失效队列处理完成，保证cache和内存中的数据是相同的。

store-store乱序不会发生在顺序处理器，在乱序处理器中屏障指令可以防止store被乱序发射至写缓冲区。原子指令不保证清空写缓冲区和失效队列，不能保证前后的指令内存序正确，因此保护数据必须依赖内存屏障。

## TickLock

原始自旋锁实现有一个问题：每个核对锁的获取是顺序限制的，这会导致潜在的饥饿问题，即某个CPU运气实在不好，那么它可能很长时间都无法占有自旋锁。

TickLock解决了这个问题。它具有两个成员：tick和cur。尝试获取锁的CPU会将tick原子递增1并获取到旧的值，这个值就是当前的tick。然后它会等待cur，当cur等于tick时就获取到锁了。解锁也非常简单，只要向cur写入tick+1就行了。

## QSpinLock

为了将自旋锁的性能压榨到极致，FTL OS实现了Linux中实现了QSpinLock来解决常规自旋锁会出现的缓存失效问题。

> 性能测试中，有竞争下的TickLock和QSpinLock的开销都是原始自旋锁的数倍，这是解锁的顺序性导致。CPU各个核心之间的通信时间是不相同的，原始自旋锁发生竞争时最近的CPU会抢到锁，而其他两种锁不能利用这一点。

上文提到了CPU多核间的内存同步使用了MESI协议，因此如果所有自旋锁都在同一个地址上spin，那么当一个核心解锁时会修改此地址，于是这个缓存块在其他核心就失效了。其他核心发现缓存块失效了就会向内存发送读取请求，而这些核心的读写请求是同时发生的，会导致拥塞现象使性能大幅下降。

为了解决这个问题，QSpinLock修改了自旋逻辑，当第三个核心尝试自旋时，它不会在锁自身上自旋，而是在一个独立的单元上自旋。这样每次解锁就只会产生一次缓存块失效，不影响其他的CPU，大幅提升多核竞争下的性能。

### 实现

QSpinLock的空间占用为32位。其中，低8位为lock，8-16位为pending，16-32位为tail。lock和pending事实上只使用了一位，但占用了8字节来保证写入的原子性，同时lock和pending必须连续且16位对齐，保证可以原子地同时写入这两位。

初始化后锁将置为0。我们用三元组(tail,pending,lock)来表示这3个数。

#### 第一个核心获取锁(0,0,0)

当三元组为(0,0,0)时说明没有上锁，此时第一个核心将通过CAS方式原子地将锁从(0,0,0)修改为(0,0,1)，内存序为Acquire。

#### 释放锁

以Release向lock位写入0，必须使用字节写入指令写入保证不会破坏tail和pending位。

#### 第二个核心获取锁(0,0,1)

当三元组为(0,0,1)时说明此核心为第二个核心。此时通过CAS操作将(0,0,1)修改为(0,1,1)。

#### 第二个核心的递进(x,1,0)

当观测到lock位为0时进入递进阶段。向(pending和lock)写入(0,1)，必须使用2字节写入指令写入保证不破坏tail位。插入Acquire的fence。

#### 第三个核心获取锁(0,1,x)

如果观测到(0,1,0)则等待它变为(0,0,1)，进入第二个核心的获取路径。当观测到(0,1,1)时进入本路径。

在本CPU缓存上初始化一个MCS节点，将next和lock置为0，并生成一个可以进入此node的id，将id以CAS更新到tail中。即：(0,1,1)变为(id,1,1)。

MCS节点放置在本CPU空间。可以在全局放置一个指针数组来索引每个缓存的位置。

#### 第三个核心的递进(x,0,0)

当第三个核心观测到(x,0,0)时说明第二个核心已经释放。检测x：

* 如果x为自身，使用CAS更新三元组为(0,0,1)。
* 如果x不为自身则存在后继结点，将锁上的lock设为1。next的值可能尚未到达，因此需要等待至next不为0。将next上的lock设为1。
* 注意更新三元组和设置lock直接需要内存屏障来防止写乱序，否则下一个节点会以为这个节点已经解锁了！

#### 第4+个核心获取锁(x,x,x)

当tail位不为0时初始化MCS节点，使用CAS操作将tail位指向自身，同时还可以获取到旧的tail: old。此时将old的next指向自身MCS节点。

#### 第4+个核心的递进 lock=1

当自身MCS节点上的lock变为1后递进。此时进入第三个核心的递进阶段，等待(x,0,0)。

### 小细节

> tail是16位的，而第N个核心需要用它来到实际的节点并修改next值。那么如何用tail来找到真实的节点？

在全局静态区开一个数组，以tail为索引。

> 自旋锁获取时可能发生中断，如果中断上下文尝试获取另一个自旋锁，怎么防止这两个自旋锁使用同一个节点？

中断的嵌套是有限制的，将自旋锁嵌套次数加入MCS节点索引。

## 顺序锁

顺序锁适用于读者远多于写者的工作状态，相比于读写锁，顺序锁运允许读者完全不需要操作任何的原子变量，因此在写者较少的情况下可以为读侧提供近似RCU的极高性能。

顺序锁允许读者和写者同时操作数据，这意味着读者读取到的数据可能是不完整的。为了解决这种情况，顺序锁引入了序列号来标识写者的操作是否完成。写者获取锁和释放锁都会将序列号增加1，同时用偶数和奇数分别表示锁未被占用和被占用的状态。这样就可以让原子变量兼顾锁和序列号的职能。读者在操作数据前后都需要读取序列号的值，只有两次读取到的序列号都相同且都为偶数才表示读取到的数据是正确的。为了防止乱序，两次序列号的读取和操作之前都需要内存屏障。

顺序锁不保证读者操作的安全性。允许读者和写者同时访问的代价就是读者操作的数据结构可能会被破坏，以至于读者访问到了非法数据段导致程序出错，因此顺序锁一般只能用于简单的数据结构。
